{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our testing environment. This will act as the MDP for our algorithm.\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_observations = env.observation_space.n  # The number of possible states of the environment.\n",
    "num_actions = env.action_space.n  # The number of available actions in each state.\n",
    "print('Possible observations:', num_observations)\n",
    "print('Available actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # The learning rate.\n",
    "gamma = 0.5  # The discount factor.\n",
    "epsilon = 0.1  # The exploration probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the main loop.\n",
    "num_train_episodes = 500  # Modify to tune for convergence\n",
    "render = True  # Set to False to speed up training\n",
    "returns = []\n",
    "# This creates our Q-table: A two-dimensional array of all zeros.\n",
    "q_table = np.zeros([num_observations, num_actions])\n",
    "print('Shape of the Q-table:', q_table.shape)\n",
    "for episode in range(1, num_train_episodes):\n",
    "    # By resetting the environment you also get the initial observation or \"state\".\n",
    "    observation = env.reset()\n",
    "\n",
    "    # Repeat until the environment is done.\n",
    "    done = False\n",
    "    episode_return = 0\n",
    "    while not done:        \n",
    "        action = env.action_space.sample() # Picks a random action (between 0 and 5).\n",
    "        # TODO: Replaced the upper line to pick the best action based on the Q-table!\n",
    "        # Don't forget to keep letting the agent explore its environment a bit with epsilon-greedy.\n",
    "\n",
    "        # Perform the chosen action in the environment to get reward values and the next observation.\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update your Q-table based on the information you gathered!\n",
    "        # TODO: This is where the magic (learning) happens.\n",
    "        \n",
    "        # Step the observation\n",
    "        observation = next_observation\n",
    "                \n",
    "        # For performance reasons, only display every 200th episode.\n",
    "        episode_return += reward\n",
    "        if render and episode % 200 == 0:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            print(\"Episode: \", episode)\n",
    "            print(\"Reward: \", reward)\n",
    "            print(\"Return so far: \", episode_return)\n",
    "            # Wait a little bit. We don't want to go too fast.\n",
    "            sleep(0.1)\n",
    "    returns.append(episode_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_return = round(np.mean(returns), 2)\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Mean return: \" +  str(mean_return))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
