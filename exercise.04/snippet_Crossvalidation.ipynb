{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Before, we used the 'train_test_split' function to perform one experimental run\n",
    "However, to use just one split often biased (you could ask: what qualifies a data point to be in the training or test set?)\n",
    "so running multiple instances using your model but with different sets each run is the key idea in crossvalidation\n",
    "In the newest version of scikit, it is sufficient to just call 'cross_val_score' with the desired number of folds. \n",
    "It returns already the accuracy when trained on each fold'''\n",
    "\n",
    "# So just import it instead of 'train_test_split'\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get some data, use X for the datapoints and y for the label (or rename but then change accordingly)\n",
    "\n",
    "# Define a classifier\n",
    "\n",
    "yourClassifier = WhateverClassifierYouWant(with_whatever_parameter)\n",
    "\n",
    "# Here, the k-fold crossvalidation starts with k=10; you do not pass separated train and test sets anymore \n",
    "scores = cross_val_score(yourClassifier, X, y, cv=10, scoring='accuracy')\n",
    "# Prints all scores of all folds\n",
    "print(scores)\n",
    "\n",
    "# Prints the average, this is actually what you want and report in a paper\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Okay, now you hopefully got a refresh on k-fold crossvalidation and how to use it practically. This technique does not\n",
    "only cope with small datasets but is especially used to find out the best hyperparameter for a model, e.g. the learning\n",
    "rate, say 'eta', in a neural network. Here is how you can do it, but be aware that you can't just run it as it is: '''\n",
    "\n",
    "# if you use integers\n",
    "param_range = list(range(?, ?)) # remember the range function from the first tutorial?\n",
    "\n",
    "# if you use floats, e.g. for the learning rate \n",
    "#eta=[(i+1)/10.0 for i in range(10)] \n",
    "#print eta\n",
    "\n",
    "fold_scores = []\n",
    "for k in param_range:\n",
    "mlp_example=MLPClassifier(learning_rate_init=k) \n",
    "    scores = cross_val_score(mlp_example, X, y, cv=10, scoring='accuracy')\n",
    "    fold_scores.append(scores.mean())\n",
    "print(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is nicer to plot it the parameter range (here for example for eta, the learning rate, but change to your needs); \n",
    "# refreshes also your skills from the 2nd tutorial :)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(param_range, fold_scores)\n",
    "plt.xlabel('parameter value')\n",
    "plt.ylabel('fold accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
